{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " StyleTransfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDbI7Yg2vC4d"
      },
      "source": [
        "Based on: https://arxiv.org/abs/1508.06576"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIQ8lGWCkndm"
      },
      "source": [
        "!ln -s \"/content/drive/MyDrive/meu/imgs\" ./data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFFw_E9M9CcK"
      },
      "source": [
        "# from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# import torch.nn.functional as F\n",
        "# import matplotlib.pyplot as plt\n",
        "# import copy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "img_size =  356\n",
        "loader = transforms.Compose(\n",
        "    [transforms.Resize((img_size,img_size)),\n",
        "     transforms.ToTensor(),\n",
        "     #transforms.Normalize(mean=[],std=[])\n",
        "     ]\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2vXU6OT_vrG"
      },
      "source": [
        "# cnn = models.vgg19(pretrained=True).to(device).eval()# get all NET\n",
        "# cnn = models.vgg19(pretrained=True).features.to(device).eval()# get only the CNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-YB0lxHh1vD"
      },
      "source": [
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "def read_img(path,device):\n",
        "    img = cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB) # inportando imagem\n",
        "    img = torch.Tensor(img) # convertendo em tensor\n",
        "    img = img.permute(2,0,1).unsqueeze(0) # reordenando\n",
        "    img = F.interpolate(img, size=224) # resize\n",
        "    # img = img.to('cpu')\n",
        "    return img.to(device)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyfC7pqOf1-b"
      },
      "source": [
        "def load_image(path,device,loader):\n",
        "    img = Image.open(path)\n",
        "    img = loader(img).unsqueeze(0)\n",
        "    return img.to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI7NIfg0d-iE"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features_layers = [0,5,10,19,28]\n",
        "        self.model = models.vgg19(pretrained=True).features[:29].to(device).eval()\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for num, layer in enumerate(self.model):\n",
        "            x=layer(x)\n",
        "\n",
        "            if num in self.features_layers :\n",
        "                features.append(x)\n",
        "        return features\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXQKzYFglM9J"
      },
      "source": [
        "### Import images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPo0g5l3jrbD"
      },
      "source": [
        "original_img = load_image('/content/data/foto.jpeg',device=device,loader=loader)\n",
        "style_img = load_image('/content/data/AP_Biblioteca_R01-960x540.jpg',device=device,loader=loader)\n",
        "generated_img = original_img.clone().requires_grad_(True)\n",
        "model = VGG(device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5UW_dDMl00a"
      },
      "source": [
        "### Hyperparametrs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSi9WrtJlKIg"
      },
      "source": [
        "steps = 600\n",
        "learn_rate = 0.01\n",
        "alpha = 0.56 # content_weight\n",
        "beta = 1000000 #0.56 # style_weight\n",
        "optimizer = optim.Adam([generated_img],lr=learn_rate)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpp7Lr2kllsY",
        "outputId": "cd27dd60-9e94-4083-cea3-a0c0cf412f5c"
      },
      "source": [
        "for step in range(steps):\n",
        "    generated_img_features = model(generated_img)\n",
        "    original_img_features = model(original_img)\n",
        "    style_img_features = model(style_img)\n",
        "\n",
        "    style_loss = original_loss = 0\n",
        "\n",
        "    for gen_feature, orig_feature, style_feature in zip(\n",
        "        generated_img_features, original_img_features, style_img_features):\n",
        "\n",
        "        bash_size, channel, h,w =  gen_feature.shape\n",
        "\n",
        "        original_loss += torch.mean((gen_feature-orig_feature)**2) \n",
        "\n",
        "        #compute gram matrix\n",
        "        G = gen_feature.view(channel,h*w).mm(\n",
        "            gen_feature.view(channel,h*w).t())\n",
        "        \n",
        "        A = style_feature.view(channel,h*w).mm(\n",
        "            style_feature.view(channel,h*w).t())\n",
        "\n",
        "        style_loss += torch.mean((G-A)**2) \n",
        "\n",
        "    total_loss = alpha * original_loss + beta * style_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"{step}: {total_loss.item()}\")\n",
        "        save_image(generated_img,f\"hoovertowernight_{step}.png\")\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 47414312435712.0\n",
            "50: 5152345948160.0\n",
            "100: 3938927247360.0\n",
            "150: 3579130413056.0\n",
            "200: 3358146691072.0\n",
            "250: 3186107875328.0\n",
            "300: 3035753349120.0\n",
            "350: 2894838628352.0\n",
            "400: 2755863511040.0\n",
            "450: 2603556536320.0\n",
            "500: 2434341011456.0\n",
            "550: 2243671621632.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLPowHgFrULw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}